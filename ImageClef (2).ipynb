{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdf33e9-f959-4171-bac9-3af2fd615acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0c1299-96a0-4eb4-8778-b41d5ab9a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the data\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    for entry in data:\n",
    "        if 'src' in entry and 'tgt' in entry:\n",
    "            input_texts.append(entry['src'])\n",
    "            target_texts.append(entry['tgt'])\n",
    "    return input_texts, target_texts\n",
    "\n",
    "train_data = load_data('train.json')\n",
    "valid_data = load_data('valid.json')\n",
    "test_data = load_data('test.json')\n",
    "\n",
    "train_input_texts, train_target_texts = preprocess_data(train_data)\n",
    "valid_input_texts, valid_target_texts = preprocess_data(valid_data)\n",
    "test_input_texts, test_target_texts = preprocess_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ff22ae-2126-4ec7-a25f-43af6229b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenize the input and target texts\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(train_input_texts)\n",
    "train_input_sequences = input_tokenizer.texts_to_sequences(train_input_texts)\n",
    "valid_input_sequences = input_tokenizer.texts_to_sequences(valid_input_texts)\n",
    "test_input_sequences = input_tokenizer.texts_to_sequences(test_input_texts)\n",
    "\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(train_target_texts)\n",
    "train_target_sequences = target_tokenizer.texts_to_sequences(train_target_texts)\n",
    "valid_target_sequences = target_tokenizer.texts_to_sequences(valid_target_texts)\n",
    "test_target_sequences = target_tokenizer.texts_to_sequences(test_target_texts)\n",
    "\n",
    "max_input_length = max(len(sequence) for sequence in train_input_sequences)\n",
    "max_target_length = max(len(sequence) for sequence in train_target_sequences)\n",
    "\n",
    "train_input_sequences = pad_sequences(train_input_sequences, maxlen=max_input_length, padding='post')\n",
    "valid_input_sequences = pad_sequences(valid_input_sequences, maxlen=max_input_length, padding='post')\n",
    "test_input_sequences = pad_sequences(test_input_sequences, maxlen=max_input_length, padding='post')\n",
    "\n",
    "train_target_sequences = pad_sequences(train_target_sequences, maxlen=max_target_length+1, padding='post')\n",
    "valid_target_sequences = pad_sequences(valid_target_sequences, maxlen=max_target_length+1, padding='post')\n",
    "test_target_sequences = pad_sequences(test_target_sequences, maxlen=max_target_length+1, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f03711e-70c9-4d74-ba56-63475fa56838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the NMT model\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_dim=len(input_tokenizer.word_index) + 1, output_dim=latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_target_length,))\n",
    "decoder_embedding = Embedding(input_dim=len(target_tokenizer.word_index) + 1, output_dim=latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "attention = Attention()([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_attention = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attention])\n",
    "decoder_dense = Dense(len(target_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_attention)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de4a59f-a53f-4f7c-a80b-80878dcea6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "190/190 [==============================] - 818s 4s/step - loss: 0.9879 - accuracy: 0.8791 - val_loss: 0.6652 - val_accuracy: 0.8877\n",
      "Epoch 2/10\n",
      "190/190 [==============================] - 837s 4s/step - loss: 0.6230 - accuracy: 0.8929 - val_loss: 0.5951 - val_accuracy: 0.8972\n",
      "Epoch 3/10\n",
      "190/190 [==============================] - 873s 5s/step - loss: 0.5659 - accuracy: 0.8998 - val_loss: 0.5595 - val_accuracy: 0.9007\n",
      "Epoch 4/10\n",
      "190/190 [==============================] - 846s 4s/step - loss: 0.5372 - accuracy: 0.9035 - val_loss: 0.5406 - val_accuracy: 0.9034\n",
      "Epoch 5/10\n",
      "190/190 [==============================] - 855s 4s/step - loss: 0.5193 - accuracy: 0.9057 - val_loss: 0.5284 - val_accuracy: 0.9048\n",
      "Epoch 6/10\n",
      "190/190 [==============================] - 859s 5s/step - loss: 0.5057 - accuracy: 0.9075 - val_loss: 0.5162 - val_accuracy: 0.9068\n",
      "Epoch 7/10\n",
      "190/190 [==============================] - 868s 5s/step - loss: 0.4883 - accuracy: 0.9102 - val_loss: 0.4973 - val_accuracy: 0.9100\n",
      "Epoch 8/10\n",
      "190/190 [==============================] - 852s 4s/step - loss: 0.4602 - accuracy: 0.9152 - val_loss: 0.4658 - val_accuracy: 0.9168\n",
      "Epoch 9/10\n",
      "190/190 [==============================] - 848s 4s/step - loss: 0.4113 - accuracy: 0.9257 - val_loss: 0.4113 - val_accuracy: 0.9293\n",
      "Epoch 10/10\n",
      "190/190 [==============================] - 884s 5s/step - loss: 0.3358 - accuracy: 0.9418 - val_loss: 0.3357 - val_accuracy: 0.9450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a32a1d04c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'], run_eagerly=True)\n",
    "model.fit([train_input_sequences, train_target_sequences[:, :-1]], train_target_sequences[:, 1:], epochs=10, batch_size=128, validation_data=([valid_input_sequences, valid_target_sequences[:, :-1]], valid_target_sequences[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af2def4-3f49-4926-8426-935162733324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "# Step 2: Make predictions on the test data\n",
    "def make_predictions(model, test_data):\n",
    "    predictions = []\n",
    "    for entry in test_data:\n",
    "        # Assuming your model.predict() function takes input data and returns predictions\n",
    "        input_data = entry.get('valid_data', None)  # Get input data from entry, or use a default value if not present\n",
    "        if input_data is not None:\n",
    "            prediction = model.predict(input_data)  # Replace 'input_data' with your actual input data key\n",
    "            predictions.append([{\"id\": entry['id'], \"hyp\": prediction}])  # Format prediction\n",
    "    return predictions\n",
    "\n",
    "# Load test data (replace 'test_data' with your actual test data)\n",
    "with open('test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Step 3: Generate predictions\n",
    "predictions = make_predictions(model, test_data)\n",
    "\n",
    "# Step 4: Write predictions to a JSON file\n",
    "def write_predictions_to_json(predictions, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(predictions, f)\n",
    "\n",
    "write_predictions_to_json(predictions, 'predictions.json')\n",
    "\n",
    "# Step 5: Archive the run file\n",
    "def archive_run_file(run_file):\n",
    "    with zipfile.ZipFile('run.zip', 'w') as zipf:\n",
    "        zipf.write(run_file)\n",
    "\n",
    "archive_run_file('predictions.json')\n",
    "\n",
    "# Provide the path to your team name and the zip file containing the predictions.json file\n",
    "team_name = \"Hat Tricks\"\n",
    "zip_file_path = \"run.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b629a47a-a40b-482c-9fec-5b8349227ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "def read_sequences_from_json(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return [entry['id'] for entry in data]\n",
    "\n",
    "def calculate_bleu_score(ref_file, hyp_file):\n",
    "    references = read_sequences_from_json(ref_file)\n",
    "    hypotheses = read_sequences_from_json(hyp_file)\n",
    "    bleu = corpus_bleu(hypotheses, [references])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79dad07b-f9c3-48c4-84a3-f5abbbf41c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 2.8653498031922857\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "ref_file = 'test.json'  # Path to reference JSON file\n",
    "hyp_file = 'run.json'  # Path to hypothesis JSON file\n",
    "bleu_score = calculate_bleu_score(ref_file, hyp_file)\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
